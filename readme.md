# ğŸ Retail Sales Analytics ETL Project (Dockerized)

This project demonstrates a **complete local ETL pipeline** using only **open-source tools**. It loads retail sales data, transforms it with DBT, and orchestrates the workflow using Apache Airflow. The entire stack runs locally via **Docker**. Now supports **PySpark** for large-scale data processing.
---

## ğŸ§± Tech Stack

- **Apache Airflow** â€“ ETL orchestration
- **PostgreSQL** â€“ Data warehouse
- **Python (Pandas + SQLAlchemy)** â€“ Data ingestion
- **DBT (Data Build Tool)** â€“ SQL-based data transformation
- **PySpark** â€“ Big data processing support
- **Docker** â€“ Local containerized setup

---

## ğŸš€ Project Structure

```
retail-etl/
â”œâ”€â”€ dags/                      # Airflow DAGs
â”‚   â””â”€â”€ ETL.py                 # Main DAG for orchestrating ETL
â”œâ”€â”€ dbt_project/               # DBT transformation project
â”‚   â”œâ”€â”€ models/                # DBT models (stg/fact)
â”‚   â”œâ”€â”€ dbt_project.yml        # DBT config
â”‚   â””â”€â”€ profiles.yml           # DBT profiles config
â”œâ”€â”€ etl/                       # Python ETL scripts
â”‚   â”œâ”€â”€ sales_load.py          # Loads sales data to PostgreSQL
â”‚   â””â”€â”€ ingest_taxi_data.py    # Loads NYC taxi data using PySpark or Pandas
â”œâ”€â”€ data/                      # Local data volume (optional)
â”‚   â”œâ”€â”€ sales.csv
â”‚   â””â”€â”€ taxi_data.parquet          
â”œâ”€â”€ docker-compose.yml         # Docker setup for all services
â”œâ”€â”€ Dockerfile                 # Custom Airflow image with PySpark
â””â”€â”€ README.md                  # Documentation

```

---

## ğŸ› ï¸ How to Set Up & Run

### ğŸ§© 1. Clone the Repository

```bash
git clone https://github.com/your-username/retail-etl.git
cd retail-etl
```

## ğŸ“¥ 2. Download Required Datasets

Ensure the following files are present in the `./data/raw/` directory:

1. `sales.csv`  
2. `taxi_data.parquet`

### Option A: Download Manually

- **sales.csv**: [UCI Online Retail Dataset](https://archive.ics.uci.edu/ml/datasets/Online+Retail) (convert to CSV format)
- **taxi_data.parquet**: Use any NYC Yellow Taxi Trip parquet file  
  Example: [NYC TLC Trip Data (Parquet)](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)

Place both files in:

```
retail-etl/data/raw/
```

### Option B: Download via script (optional)

Add this in a script if you want to automate download using `wget` or `requests`.

---

## ğŸ³ 3. Build and Start Docker Containers

```bash
docker-compose up --build -d
```

---

## ğŸŒ 4. Access Airflow UI

Visit:

```
http://localhost:8080
```

Login with:

```
Username: admin
Password: <check container logs>
```

---
## ğŸ“Š Workflow Steps

### 1. **ETL Scripts** (`etl/`)

- `sales_load.py`: Cleans and loads retail sales data into `data_etl.sales_clean`
- `ingest_taxi_data.py`: Ingests NYC Taxi data to `data_etl.raw_taxi_data` using Pandas or PySpark

### 2. **DBT Transformations**

- `stg_sales`, `fact_sales`
- `stg_taxi_data`, `fact_trips`

### 3. **Airflow DAG**

- Orchestrates both ETL scripts and triggers DBT models

---

## âš™ï¸ Docker Commands

```bash
docker ps                             # Running containers
docker exec -it airflow_container bash  # Shell into airflow
docker exec dbt_container dbt run --project-dir /usr/app  # Trigger dbt models
docker-compose down && docker-compose up --build -d       # Rebuild and restart
```
## ğŸ”„ Restart the Entire Stack

```bash
docker-compose down
docker-compose up --build -d
```


---

## ğŸ—ƒï¸ PostgreSQL Connection Info

```
Host: localhost
Port: 5432
Username: airflow
Password: airflow
Database: analytics
```

Use pgAdmin, DBeaver, or `psql` for querying.

---

## ğŸš€ Fork & Build Something Amazing

This entire **ETL architecture is ready to use**:

- ğŸ§© Fork and plug in your data  
- ğŸ’» Works 100% locally  
- â˜ï¸ Zero cloud dependency  
- ğŸ§ª No hidden cost  
- ğŸ” Easily extendable & open-source  

---


## âœ… Future Enhancements

- BI dashboard (Superset / Metabase)
- dbt incremental models
- Real-time API ingestion
- ML model orchestration via Airflow

---

## ğŸ™Œ Credits

Built by **Yash Shah**, inspired by Engineer

---

## ğŸ“„ License

MIT License. Use freely for learning or showcasing your portfolio.

---

Happy Data Engineering! ğŸ› ï¸ğŸ“ˆ

