# ğŸ›’ Retail Sales Analytics ETL Project (Dockerized)

This project demonstrates a **complete local ETL pipeline** using only **open-source tools**. It loads retail sales data, transforms it with DBT, and orchestrates the workflow using Apache Airflow. The entire stack runs locally via **Docker**.

---

## ğŸ§± Tech Stack

- **Apache Airflow** â€“ ETL orchestration
- **PostgreSQL** â€“ Data warehouse
- **Python (Pandas + SQLAlchemy)** â€“ Data ingestion
- **DBT (Data Build Tool)** â€“ SQL-based data transformation
- **Docker** â€“ Local containerized setup

---

## ğŸš€ Project Structure

```
retail-etl/
â”œâ”€â”€ dags/                      # Airflow DAGs
â”‚   â””â”€â”€ retail_etl.py          # Main DAG for orchestrating ETL
â”œâ”€â”€ dbt_project/               # DBT transformation project
â”‚   â”œâ”€â”€ models/                # DBT models (stg/fact)
â”‚   â”œâ”€â”€ dbt_project.yml        # DBT config
â”‚   â””â”€â”€ profiles.yml           # DBT profiles config
â”œâ”€â”€ etl/                       # Python ETL scripts
â”‚   â””â”€â”€ sal;es_load.py         # Loads data from API to PostgreSQL
â”œâ”€â”€ data/                      # Local data volume (optional)
â”‚   â””â”€â”€ raw/sales.csv          # Example static data (if needed)
â””â”€â”€â”€ docker-compose.yml        # Docker setup for all services

```

---

## ğŸ› ï¸ How to Set Up & Run

### ğŸ§© 1. Clone the Repository

```bash
git clone https://github.com/your-username/retail-etl.git
cd retail-etl
```

### ğŸ³ 2. Build and Start Docker Containers

```bash
docker-compose up --build -d
```


### ğŸŒ 3. Access Airflow UI

Open your browser:

```
http://localhost:8080
```

Login with:

```
username: admin
password: <get it from logs>
```

---

## ğŸ“Š Workflow Steps

### 1. **Python ETL Script** (`etl/load.py`)

- Fetches  data from `sales.csv` 
- Cleans & transforms data
- Loads into `data_etl.sales_clean` in PostgreSQL

### 2. **DBT Transformations** (`dbt_project/`)

- `stg_sales`: staging layer
- `fact_sales`: deduplicated, aggregated layer

### 3. **Airflow DAG**

- Executes the `sales_load.py` script
- Runs DBT transformations

---

## âš™ï¸ Common Docker Commands

### ğŸ“¦ View Running Containers

```bash
docker ps
```

### ğŸš Shell into a Container

```bash
docker exec -it airflow_container bash
```

### ğŸ§ª Run DBT from dbt\_container

```bash
docker exec dbt_container dbt run --project-dir /usr/app
```

### ğŸ”„ Restart the Entire Stack

```bash
docker-compose down
docker-compose up --build -d
```

---

## ğŸ—ƒï¸ PostgreSQL Connection Info

```
Host: localhost
Port: 5432
Username: airflow
Password: airflow
Database: retail_db
```

You can connect using DBeaver, pgAdmin, or `psql`.

---

## âœ… Future Enhancements

- Add Superset/Metabase for BI
- Implement dbt incremental models
- Use real-time API for incremental loads
- Add data tests and docs (`dbt test`, `dbt docs`)

---

## ğŸ™Œ Credits

Built by Yash Shah

---

## ğŸ“„ License

MIT License. Use freely for learning or showcasing your portfolio.

---

Happy Data Engineering! ğŸ› ï¸ğŸ“ˆ

